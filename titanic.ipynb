{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle's Titanic Data Science Challenge\n",
    "\n",
    "This notebook is a walkthrough a basic workflow for solving data science competitions at sites like Kaggle.\n",
    "\n",
    "**Resources**:\n",
    "- [Kaggle - Titanic](https://www.kaggle.com/c/titanic).\n",
    "- [Kaggle - Titanic dataset](https://www.kaggle.com/competitions/titanic/data).\n",
    "- [Kaggle - Titanic Data Science Solutions](https://www.kaggle.com/code/startupsci/titanic-data-science-solutions/notebook).\n",
    "- [YouTube - Beginner Kaggle Data Science Project Walk-Through (Titanic)](https://www.youtube.com/watch?v=I3FBJdiExcg).\n",
    "- [YouTube - Kaggle Titanic Survival Prediction Competition Part 1/2](https://www.youtube.com/watch?v=GSk-EEu1zkA&t=31s).\n",
    "- [YouTube - Kaggle Titanic Survival Prediction Competition Part 2/2](https://www.youtube.com/watch?v=i5E2hruuLaQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for data analysis.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for visualizations.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for machine learning.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire data.\n",
    "\n",
    "training_df = pd.read_csv('./data/train.csv')\n",
    "testing_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into the training dataframe.\n",
    "\n",
    "training_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into the testing dataframe.\n",
    "\n",
    "testing_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview training dataframe.\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of each feature in the training dataframe.\n",
    "\n",
    "training_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of each feature in the test dataframe.\n",
    "\n",
    "testing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of categorical and non-categorial features\n",
    "\n",
    "How representative is the training dataset of the actual problem domain?\n",
    "* The total samples are 891 or 40% of the actual number of passengers on board of the Titanic.\n",
    "* Survived is a categorical feature with 0 or 1 values.\n",
    "* Around 38% samples survived representative of the actual susrvival rate at 32%.\n",
    "* Most passengers (>75%) did not travel with parents or children.\n",
    "* Fared varied significantly with few passengers (< 1%) paying as high as $512.\n",
    "* Few elderly passengers (<1%) withing age range 65-80.\n",
    "\n",
    "What is the distribution of categorical features?\n",
    "- Names are unique across the dataset.\n",
    "- Sex variable as two possible values with 65% males.\n",
    "- Cabin values have several duplicates across samples (several passengers shared a cabin).\n",
    "- Embarked takes three possible values depending on the port.\n",
    "- Ticket feature has high ratio (22%) of duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions based on the data analysis\n",
    "\n",
    "> The following assumptions are validated further before taking appropriate actions.\n",
    "\n",
    "**Completing**:\n",
    "1. Complete the Age feature sice its definitely correlated to survival.\n",
    "1. Complete the Embarked feature since its also correlated with survival.\n",
    "\n",
    "**Correcting**:\n",
    "1. Ticker feature is dropped since it contains a high ratio of duplicates and there may not be a correlation between Ticker and survival.\n",
    "1. Cabin feature is dropped as it is highly incomplete or contains many `Null`s.\n",
    "1. PassengerId is dropped from training dataset as it doesn't contribute to survival.\n",
    "1. Name feature is relatively non-standard and may not contribute directly to survival, so maybe its dropped.\n",
    "\n",
    "**Creating**:\n",
    "1. Create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n",
    "1. Engineer the Name feature to extract Title as a new feature.\n",
    "1. Create a new feature for Age bands.\n",
    "1. Create a Fare range feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe signigicant correlation (>0.5) among Pclass = 1 and Survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirm that Sex=female had very high survival rate at 74%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_df[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, SibSp and Parch features have zero correlation for certain values. So it may be best to derive a feature or a set of features from these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "A histogram chart is useful for analyzing continuous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distributions of samples using automatically defined bins or equally ranged bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(training_df, col='Survived')\n",
    "grid.map(plt.hist, 'Age', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(\n",
    "    training_df,\n",
    "    col='Survived',\n",
    "    row='Pclass',\n",
    "    height=2.2,\n",
    "    aspect=1.6,\n",
    ")\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlating categorical features\n",
    "\n",
    "No we can correlate categorical features with our solution goal.\n",
    "\n",
    "**Observations**\n",
    "- Female passengers had much better survival rate than males.\n",
    "- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n",
    "- Males had better survival rate in Pclass=3 compared with Pclass=2 for C and Q ports.\n",
    "- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers.\n",
    "\n",
    "**Decisions**\n",
    "- Add Sex feature to model training.\n",
    "- Complete and add Embarked feature to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(\n",
    "    training_df,\n",
    "    row='Embarked',\n",
    "    height=2.2,\n",
    "    aspect=1.6\n",
    ")\n",
    "grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to correlate categorical features (wit non-numeric values) and numeric signatures. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Higher fare paying passengers had better survival.\n",
    "- Port of embarkation correlates with survival rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(\n",
    "    training_df,\n",
    "    row='Embarked',\n",
    "    col='Survived',\n",
    "    height=2.2,\n",
    "    aspect=1.6\n",
    ")\n",
    "grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, errorbar=None)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping features\n",
    "Based on our assumptions we're going to drop the Cabin and Ticket features.\n",
    "\n",
    "> We're going to do both operations for the training and testing datasets to stay consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "testing_df = testing_df.drop(['Ticket', 'Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new features\n",
    "\n",
    "We want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n",
    "\n",
    "**Observations**:\n",
    "- Most titles band Age groups accurately.\n",
    "- Survival among Title bands varies slightly.\n",
    "- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev,...)\n",
    "\n",
    "**Decisions**:\n",
    "- Retain the Title feature for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['Title'] = training_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "testing_df['Title'] = testing_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "pd.crosstab(training_df['Title'], training_df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace titles with a more common name.\n",
    "\n",
    "for dataset in [training_df, testing_df]:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "training_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical titles to ordinal.\n",
    "\n",
    "title_map = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
    "\n",
    "for dataset in [testing_df, training_df]:\n",
    "    dataset['Title'] = dataset['Title'].map(title_map)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Name feature from testing and training datasets and\n",
    "# PassengerId from training.\n",
    "\n",
    "training_df = training_df.drop(['Name', 'PassengerId'], axis=1)\n",
    "testing_df = testing_df.drop(['Name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also convert categorical features to numerical values. This is required by most algorithms and doing so also help us achieving the feature completing goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Sex feature.\n",
    "\n",
    "for dataset in [training_df, testing_df]:\n",
    "    dataset['Sex'] = dataset['Sex'].map({'female': 1, 'male': 0}).astype(int)\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing numerical continuous feature\n",
    "\n",
    "We should start estimating and completing features with missing or null values. The first feature will be the Age feature.\n",
    "\n",
    "The method considered consists on using other correlated features. In our case we note correlation among Age, Gender and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=! and Gender=0, Pclass=1 and Gender=1, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(training_df, row='Pclass', col='Sex', height=2.2, aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing an empty array to contain guessed Age values\n",
    "# based on Pclass times Gender combinations.\n",
    "\n",
    "guess_ages = np.zeros((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over Sex and Pclass to calculate guessed values of \n",
    "# Age for the six combinations.\n",
    "\n",
    "for dataset in [training_df, testing_df]:\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 3):\n",
    "            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j + 1)]['Age'].dropna()\n",
    "\n",
    "            age_guess = guess_df.median()\n",
    "\n",
    "            # Conver random age float to nearest .5 age.\n",
    "            guess_ages[i, j] = int(age_guess/0.5 + 0.5) * 0.5\n",
    "\n",
    "    for i in range(0, 2):\n",
    "        for j in range (0, 3):\n",
    "            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j + 1), 'Age'] = guess_ages[i, j]\n",
    "\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Age bands and determine correlations with Survived.\n",
    "\n",
    "training_df['AgeBand'] = pd.cut(training_df['Age'], 5)\n",
    "training_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Age ordinals based on bands.\n",
    "\n",
    "for dataset in [training_df, testing_df]:\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove AgeBand feature.\n",
    "\n",
    "training_df = training_df.drop(['AgeBand'], axis=1)\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create now a new feature for FamilySize which combines Parch and SibSp. This will enable to drop both from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [training_df, testing_df]:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "training_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IsAlone feature.\n",
    "\n",
    "for dataset in [testing_df, training_df]:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "training_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Parch, SibSp, and FamilySize in favor of IsAlone.\n",
    "\n",
    "training_df = training_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
    "testing_df = testing_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the categorical Embarked feature that takes S, Q and C values based on port of embarkation we will simply fill these with the most common occurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_port = training_df.Embarked.dropna().mode()[0]\n",
    "freq_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [training_df, testing_df]:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n",
    "\n",
    "training_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also convert the Embarked feature into a numeric feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "\n",
    "for dataset in [testing_df, training_df]:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map(embarked_map)\n",
    "\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now complete the Fare feature for single missing value in the test dataset using mode to get the value that occurs most frequently for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df['Fare'].fillna(testing_df['Fare'].dropna().median(), inplace=True)\n",
    "\n",
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FareBand feature.\n",
    "\n",
    "training_df['FareBand'] = pd.qcut(training_df['Fare'], 4)\n",
    "training_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Fare feature to orfinal values based on FareBand.\n",
    "\n",
    "for dataset in [testing_df, training_df]:\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "training_df = training_df.drop(['FareBand'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Since our problem is a classification and regression problem. We want to identify relationship between ouput (Survived or not) with other varibales or features. We are going to use supervised learning to training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a useful model to run early in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression.\n",
    "\n",
    "X_train = training_df.drop(\"Survived\", axis=1)\n",
    "Y_train = training_df[\"Survived\"]\n",
    "X_test  = testing_df.drop(\"PassengerId\", axis=1).copy()\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficient of the features.\n",
    "\n",
    "coeff_df = pd.DataFrame(training_df.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df['Correlation'] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n",
    "\n",
    "**Observations**:\n",
    "- Sex is highest positive coefficient, implying that as the Sex feature increases, the probability of Survived=1 increases the most.\n",
    "- Inversely as Pclass increases, probability of Survived=1 decreases the most.\n",
    "- Age*Class is a good artifical feature to model as it has second highest negative correlation with Survived.\n",
    "- Titles is the second highest positive correlation and Embarked the third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector machines\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = svc.predict(X_test)\n",
    "\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN score.\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naïve Bayes.\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "Y_pred = gaussian.predict(X_test)\n",
    "\n",
    "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
    "acc_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron.\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = perceptron.predict(X_test)\n",
    "\n",
    "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC.\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = linear_svc.predict(X_test)\n",
    "\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent.\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = sgd.predict(X_test)\n",
    "\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desicion Tree.\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest.\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision tree's habit of overfitting to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naïve Bayes', 'Perceptron', 'Stochastic Gradient Decent', 'Linear SVC', 'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
    "\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'PassengerId': testing_df['PassengerId'],\n",
    "    'Survived': Y_pred,\n",
    "})\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export.\n",
    "\n",
    "submission.to_csv('./output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "242b79aae9e5abae5489f3cba317aa0adeea1765ac7131b3f67ae9012b212271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
